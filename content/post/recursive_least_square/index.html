---
authors:
- admin
date: "2020-01-30T00:00:00Z"
draft: false
featured: false
image:
  caption: 'Image credit: [**vuejsdevelopers**](https://vuejsdevelopers.com/images/posts/recursive_components.jpg)'
  focal_point: ""
  placement: 2
  preview_only: false
lastmod: "2020-01-30T00:00:00Z"
projects: [statistics]
subtitle: 'from least square estimation to recursive least square estimation'
summary: This is a journey from linear regression to recursive linear regression
title: 'Recursive Least Squares Estimation'
categories:
  - r programming
  - linear model
tags:
  - recursive least square
---



<div id="mango-again" class="section level2">
<h2>Mango again</h2>
<p>In <a href="https://www.theissaclee.com/post/weighted_linear_regression/">the previous post</a>, I explained the weighted least squares estimation using the example of measuring the weight of a mango. In this post, I will use a similar setup, but a slightly different one.</p>
<p>I bought a mango from a grocery store and measured its weight using my scale. However, since my scale is cheap, it has its own bias, and a random noise whose expectation is zero and the standard deviation is 1g. Every time I check the weight of mango, I have checked the values of the scale before I put mango on it, I got these values.</p>
<pre><code>## [1] -0.1035329  0.6387146  1.0422206 -0.6728489  0.7145623  0.7530279  0.2126300</code></pre>
<p>Note that these values include the bias of my scale and the random noises. After measuring the weight of the mango seven times, I got these values again.</p>
<pre class="r"><code>weight_mango &lt;- c(536.5859, 539.5549, 541.1689, 534.3086, 539.8582, 540.0121, 537.8505)
weight_mango</code></pre>
<pre><code>## [1] 536.5859 539.5549 541.1689 534.3086 539.8582 540.0121 537.8505</code></pre>
<p>Next, let us denote the two parameters the bias of my scale and weight of mango by <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p><span class="math display">\[
\beta=\left(\begin{array}{c}
\beta_{0}\\
\beta_{1}
\end{array}\right)
\]</span></p>
<p>Since I have eight measurements, my observation vector <span class="math inline">\(y\)</span> is as follow;</p>
<pre class="r"><code>y &lt;- c(bias, weight_mango)
y</code></pre>
<pre><code>##  [1]  -0.1035329   0.6387146   1.0422206  -0.6728489   0.7145623   0.7530279
##  [7]   0.2126300 536.5859000 539.5549000 541.1689000 534.3086000 539.8582000
## [13] 540.0121000 537.8505000</code></pre>
<p>Then, we can model the situation using data matrix <span class="math inline">\(X\)</span>, observation <span class="math inline">\(y\)</span>, and a random noise vector <span class="math inline">\(w\)</span> as follows:</p>
<p><span class="math display">\[
y=X\beta+w
\]</span>
where the data matrix <span class="math inline">\(X\)</span> is</p>
<pre class="r"><code>X &lt;- cbind(rep(1, 14), c(rep(0, 7), rep(1, 7)))
X</code></pre>
<pre><code>##       [,1] [,2]
##  [1,]    1    0
##  [2,]    1    0
##  [3,]    1    0
##  [4,]    1    0
##  [5,]    1    0
##  [6,]    1    0
##  [7,]    1    0
##  [8,]    1    1
##  [9,]    1    1
## [10,]    1    1
## [11,]    1    1
## [12,]    1    1
## [13,]    1    1
## [14,]    1    1</code></pre>
<p>We know that ordinary least square estimation of the parameter vector <span class="math inline">\(\beta\)</span> can be calculated as,</p>
<p><span class="math display">\[
\hat{\beta}=\left(X^{T}X\right)^{-1}X^{T}y
\]</span></p>
<p>In <code>R</code>, we can obtain these using <code>lm</code> function.</p>
<pre class="r"><code>beta_hat &lt;- lm(y ~ X + 0)$coefficients
beta_hat</code></pre>
<pre><code>##          X1          X2 
##   0.3692534 538.1077609</code></pre>
</div>
<div id="recursive-least-square-estimation" class="section level2">
<h2>Recursive least square estimation</h2>
<p>What is the recursive least square estimation, and why do we need it? The problem of calculating beta estimates in the previous section that we have to calculate the inverse of the whole data matrix <span class="math inline">\(X^TX\)</span>. When we have more variables in the data set, this matrix will become larger and larger since its dimension is <span class="math inline">\(p \times p\)</span> where <span class="math inline">\(p\)</span> is the number of variables we have. In this big data era, we often have a large <span class="math inline">\(p\)</span>. So, what if we have one more observation <span class="math inline">\(y_{new}\)</span> and we want to embrace this new information to our beta estimation. Should we update the data matrix <span class="math inline">\(X\)</span> and redo the whole process of the least square estimation again? It is redundant when we think about our <code>beta_hat</code> has the information of the past data already. Can we update the little information to the information that we already have? This is when the recursive least square estimation comes into play.</p>
<p>When we say recursive, we are considering the situation that observes the sample again and again. Let us assume that I doubted my estimation, so I measured the weight of mango again.</p>
<pre class="r"><code>new_y &lt;- 538.7267
new_y</code></pre>
<pre><code>## [1] 538.7267</code></pre>
<p>Now, when I think about the information I have so far, I have <code>new_y</code> and my previous estimation of beta <code>beta_hat</code>. Can I estimate the beta again combining this two information? With statistical notations, the situation can be expressed as follows;</p>
<p><span class="math display">\[
\begin{align*}
y_{new} &amp; =\left(\begin{array}{cc}
1 &amp; 1\end{array}\right)\left(\begin{array}{c}
\beta_{0}\\
\beta_{1}
\end{array}\right)+w_{new}\\
\hat{\beta}_{new} &amp; =\hat{\beta}_{old}+K\left(y_{new}-\hat{y}_{new}^{(old)}\right)\\
 &amp; =\hat{\beta}_{old}+K\left(y_{new}-\left(\begin{array}{cc}
1 &amp; 1\end{array}\right)\hat{\beta}_{old}\right)
\end{align*}
\]</span>
where <span class="math inline">\(\hat{y}_{new}^{old}\)</span> can be interpreted as the prediction value of our new observation, <span class="math inline">\(y_{new}\)</span>, with old information of beta. Thus, using the recursive least square method, <strong>we want to update the estimation of beta by adding the prediction error value for the new observation with the weight of <span class="math inline">\(K\)</span></strong>. So, how do we determine <span class="math inline">\(K\)</span>? Why the recursive least square follow this specific update form?</p>
</div>
<div id="unbiasness" class="section level2">
<h2>Unbiasness</h2>
<p>First, if we use this form of update equation, we can confirm that our update of the estimate of the beta will be unbiased. Note, our <span class="math inline">\(\hat{\beta_{old}}\)</span> is the least square estimation of beta, which means we assume it is unbiased estimator as follows;</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[\hat{\beta}_{old}\right] &amp; =\mathbb{E}\left[\left(X^{T}X\right)^{-1}X^{T}y\right]\\
 &amp; =\left(X^{T}X\right)^{-1}X^{T}\mathbb{E}\left[y\right]\\
 &amp; =\left(X^{T}X\right)^{-1}X^{T}\left(X\beta+\mathbb{E}\left[w\right]\right)\\
 &amp; =\beta
\end{align*}
\]</span></p>
<p>Now, if our old estimator is unbiased, our new updated estimator <span class="math inline">\(\hat{\beta}_{new}\)</span> will be unbiased regardless of <span class="math inline">\(K\)</span> as follows;</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[\hat{\beta}_{new}\right] &amp; =\mathbb{E}\left[\hat{\beta}_{old}+K\left(y_{new}-\hat{y}_{new}^{(old)}\right)\right]\\
 &amp; =\beta+K\mathbb{E}\left[\left(y_{new}-\hat{y}_{new}^{(old)}\right)\right]\\
 &amp; =\beta+K\left(\left(\begin{array}{cc}
1 &amp; 1\end{array}\right)\beta-\left(\begin{array}{cc}
1 &amp; 1\end{array}\right)\mathbb{E}\left[\hat{\beta}_{old}\right]\right)\\
 &amp; =\beta
\end{align*}
\]</span></p>
</div>
<div id="minium-variance" class="section level2">
<h2>Minium variance</h2>
<p>Thus, in the recursive least square method, we want to make our estimator have one more good property, which is the minimum variance.</p>
<p><span class="math display">\[
\begin{align*}
var\left(\hat{\beta}_{new}\right) &amp; =var\left(\hat{\beta}_{old}+K\left(y_{new}-\hat{y}_{new}^{(old)}\right)\right)\\
 &amp; =var\left(\left(I-Kx\right)\hat{\beta}_{old}+K\left(y_{new}\right)\right)\\
 &amp; =\left(I-Kx\right)var\left(\hat{\beta}_{old}\right)\left(I-Kx\right)^{T}+Kvar\left(w_{new}\right)K^{T}
\end{align*}
\]</span>
where <span class="math inline">\(x\)</span> represents the data vector <span class="math inline">\((1, 1)^T\)</span>. If we denote the covariance matrix of the estimator and random noise vector as <span class="math inline">\(var(\hat{\beta}_{old}):=P_{old}\)</span>, <span class="math inline">\(var(\hat{\beta}_{new}):=P_{new}\)</span>, and <span class="math inline">\(var(w_{new}):=R_{new}\)</span> respectively, the equation above can be re-written as follows;</p>
<p><span class="math display">\[
P_{new}=\left(I-Kx\right)P_{old}\left(I-Kx\right)^{T}+KR_{new}K^{T}
\]</span></p>
<p>If we take a trace of the <span class="math inline">\(P_{new}\)</span>, that is the summation of the variance of each estimator of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Thus, we can use this as a criterion for finding an optimal <span class="math inline">\(K\)</span>. So the objective function <span class="math inline">\(L(K)\)</span> will be</p>
<p><span class="math display">\[
L\left(K\right)=tr\left(P_{new}\right),
\]</span>
and our estimation of <span class="math inline">\(K\)</span> can be obtained after taking derivative of function <span class="math inline">\(L\)</span> with respect of <span class="math inline">\(K\)</span> and set it as zero, then solve it for <span class="math inline">\(K\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial L\left(K\right)}{\partial K} &amp; =\frac{\partial tr\left(P_{new}\right)}{\partial K}\\
 &amp; =\frac{\partial tr\left(\left(I-Kx\right)P_{old}\left(I-Kx\right)^{T}\right)}{\partial K}+\frac{\partial tr\left(KR_{new}K^{T}\right)}{\partial K}\\
 &amp; =2\left(I-Kx\right)P_{old}\left(-x^{T}\right)+2KR_{new}\overset{set}{=}0
\end{align*}
\]</span>
Note that we use the derivative formula, <span class="math inline">\(\frac{\partial tr(ABA)}{\partial A}=2AB\)</span> when <span class="math inline">\(B\)</span> is symmetric. If we solve the above equation for <span class="math inline">\(K\)</span>, we have the following;</p>
<p><span class="math display">\[
K=P_{old}x^{T}\left(xP_{old}x^{T}+R_{new}\right)^{-1}
\]</span></p>
<p>Moreover, if we assume our old estimator is the least square estimator, then the <span class="math inline">\(P_{old}\)</span> can be calculated as follows;</p>
<p><span class="math display">\[
\begin{align*}
P_{old} &amp; =var\left(\hat{\beta}_{old}\right)\\
 &amp; =var\left(\left(X^{T}X\right)^{-1}X^{T}y\right)\\
 &amp; =\left(X^{T}X\right)^{-1}X^{T}var\left(y\right)X\left(X^{T}X\right)^{-1}\\
 &amp; =\left(X^{T}X\right)^{-1}X^{T}\left(\sigma^{2}I\right)X\left(X^{T}X\right)^{-1}\\
 &amp; =\sigma^{2}\left(X^{T}X\right)^{-1}
\end{align*}
\]</span>
where <span class="math inline">\(\sigma^2\)</span> is the variance of the random noise <span class="math inline">\(w\)</span>. Thus, in our mango case, we have <span class="math inline">\(\sigma^2\)</span> is 1,</p>
<pre class="r"><code>P_old &lt;- 1 * solve(t(X) %*% X)
P_old</code></pre>
<pre><code>##            [,1]       [,2]
## [1,]  0.1428571 -0.1428571
## [2,] -0.1428571  0.2857143</code></pre>
<p>Note that we have already calculated this <code>P_old</code> for estimating beta before. Thus, the <span class="math inline">\(K\)</span> can be calculated by the following;</p>
<pre class="r"><code>x &lt;- matrix(c(1, 1), nrow = 1)
matK &lt;- P_old %*% t(x) %*% solve(x %*% P_old %*% t(x) + 1)
matK</code></pre>
<pre><code>##       [,1]
## [1,] 0.000
## [2,] 0.125</code></pre>
<p>The final update for our beta will be</p>
<p><span class="math display">\[
\begin{align*}
y_{new} &amp; =\left(\begin{array}{cc}
1 &amp; 1\end{array}\right)\left(\begin{array}{c}
\beta_{0}\\
\beta_{1}
\end{array}\right)+w_{new}\\
\hat{\beta}_{new} &amp; =\hat{\beta}_{old}+K\left(y_{new}-\hat{y}_{new}^{(old)}\right)\\
 &amp; =\hat{\beta}_{old}+K\left(y_{new}-\left(\begin{array}{cc}
1 &amp; 1\end{array}\right)\hat{\beta}_{old}\right)
\end{align*}
\]</span></p>
<pre class="r"><code>beta_hat_new = beta_hat + matK %*% (new_y - x %*% beta_hat)
beta_hat_new</code></pre>
<pre><code>##             [,1]
## [1,]   0.3692534
## [2,] 538.1389716</code></pre>
<p>Is this the same result of using the least square method? Yes.</p>
<pre class="r"><code>y &lt;- c(y, new_y)
X &lt;- rbind(X, c(1, 1))
solve(t(X) %*% X) %*% t(X) %*% y</code></pre>
<pre><code>##             [,1]
## [1,]   0.3692534
## [2,] 538.1389716</code></pre>
</div>
<div id="recursive-least-square-algorithm" class="section level2">
<h2>Recursive least square algorithm</h2>
<p>So far, we use <code>old</code> and <code>new</code> notation, but now let us switch to general <span class="math inline">\(k\)</span> time points. When we have an estimator at time <span class="math inline">\(k-1\)</span>, for time <span class="math inline">\(k\)</span>, the recursive least square estimator updates is as follows;</p>
<p><span class="math display">\[
\begin{align*}
P_{k} &amp; =\left(I-K_{k}x_{k}\right)P_{k-1}\left(I-K_{k}x_{k}\right)^{T}+K_{k}R_{k}K_{k}^{T}\\
K_{k} &amp; =\left(x_{k}P_{k-1}x_{k}^{T}+R_{k}\right)^{-1}P_{k-1}x_{k}^{T}\\
\hat{\beta}_{k} &amp; =\hat{\beta}_{k-1}+K_{k}\left(y_{k}-x_{k}\hat{\beta}_{k-1}\right)
\end{align*}
\]</span></p>
<p>Therefore, the recursive least square allows estimating the beta recursively. It will give us computational convenience, but we need to keep updating our P matrix and K matrix for each time <span class="math inline">\(k\)</span>.</p>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<ol style="list-style-type: decimal">
<li>Simon, D. (2006). Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley &amp; Sons.</li>
</ol>
</div>
