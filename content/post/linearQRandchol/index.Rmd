---
authors:
- admin
date: "2019-10-01T00:00:00Z"
draft: false
featured: false
image:
  caption: 'Image credit: [UIowaStat](https://stat.uiowa.edu/)'
  focal_point: ""
  placement: 2
  preview_only: true
lastmod: "2019-10-01T00:00:00Z"
projects: [statistics]
subtitle: 'gmp package, Cholesky and QR decomposition'
summary: Calculate the coefficients of the linear regression as a solution of the Normal equation.
title: 'The Coefficients of Linear Regression in R'
categories:
  - R
  - machine learning
  - linear model
tags:
  - gmp
  - cholesky decomposition
  - QR decomposition
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

This article is also one of my portfolio articles, but I should note that this article is written based on a wornderful course that I took at the University of Iowa: [the intensive computing course](http://homepage.divms.uiowa.edu/~luke/classes/STAT7400/).

The following are the `R` packages used in this post.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(tibble)
```

```{r message=FALSE, warning=FALSE}
library(datasets)
library(magrittr)
library(microbenchmark)
library(gmp)
```

## Normal equation for the beta vector

Today, I want to talk little bit about the process of getting the coefficients of a linear model in `R`. Among many methods, our main focus for today is related to the Cholesky decomposition and QR decomposition.

The reason why we are discussing matrix decomposition methods for linear regression is that the coefficients of the regression are the solution of the matrix equation called the Normal equation.

Suppose we have the following $n$ observations in a vector $y$ and the data matrix $X$. If we assume that the vector $y$ follows a linear model with noise vector $\mathbf{e}$, then the situation can be written as follows:

$$
\begin{eqnarray*}
\mathbf{y}_{n\times1} & = & X_{n\times p}\boldsymbol{\beta}_{p\times1}+\mathbf{e}_{n\times1}\\
 & = & \left[\begin{array}{ccccc}
\mathbf{1} & \mathbf{x}_{1} & \mathbf{x}_{2} & ... & \mathbf{x}_{p-1}\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{p-1}
\end{array}\right]+\mathbf{e}\\
 & = & \beta_{0}\mathbf{1}+\beta_{1}\mathbf{x}_{1}+\beta_{2}\mathbf{x}_{2}+...+\beta_{p-1}\mathbf{x}_{p-1}+\mathbf{e}
\end{eqnarray*}
$$

Note that $\mathbf{1}$, $\mathbf{e}$ and $\mathbf{x}_{i}$, $i=1,...,p-1$ are $n \times 1$ vectors.

Under this assumption, the coefficients of the linear model, $\beta$, which minimize the residual sum of squares (RSS), can be obtained by solving the following Normal eqaution:

$$
X^{T}X\mathbf{\beta}=X^{T}\mathbf{y}
$$

The details about the Normal equation can be found at [wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Derivation_directly_in_terms_of_matrices).

## `longley` data set

<div align="center">
![The longley data in the paper](https://raw.githubusercontent.com/issactoast/EnBlog/master/static/img/longley.png){width=560}
</div>

The Longley data set belongs to the appendix of the paper written by J. W. Longley (1967): *An appraisal of least-squares programs from the point of view of the user*. The data set is toy data and consists of 7 explanatory variables. This data is famous for having high correlation among the variables when we regress the `Employed` variable on the rest of the variables. The data set is contained in the `datasets` package, so let us load the data set.

```{r eval=FALSE}
library(datasets)
longley
```

```{r results = "asis"}
kable(longley, format = "html") %>% 
  kable_styling() %>% 
  scroll_box(width = "100%", height = "300px")
```

We can see that the `longley` data set in `R` was scaled compared to the original data.


## Calculating accurate coefficients using `gmp` package

The `gmp` package uses fractions for its calculation instead of using decimal points. This package allows us to achieve more accurate estimates for the coefficients since it avoids the truncation of floating numbers. By using the `solve()` function, let us calculate accurate coefficients for the linear regression of the `Employed` variable on the rest of the variables in the `longley` data set.

```{r }
y <- as.vector(longley[,7])
X <- as.matrix(cbind(constant = 1, longley[,-7]))

# install.packages("gmp")
# library(gmp)

# X, y rationals
r_X <- as.bigq(round(1000 * X)) / as.bigq(1000)
r_y <- as.bigq(round(1000 * y)) / as.bigq(1000)
head(r_X)
```


```{r}
# coefficients
cef_exact <- as.double(solve(t(r_X) %*% r_X,
                             (t(r_X) %*% r_y)))
cef_exact
```

## Cholesky decomposition

If the matrix $A$ satisfies $x^T \mathbf{A} x > 0$ for every $x$, then the matrix $A$ is a positive definite. 

The Cholesky decomposition can be applied to symmetric positive definite matrices. If a matrix $A$ is a symmetirc positive definite matrix then there exists a lower triangular matrix, $L$, such that

$$
\mathbf{A} = \mathbf{L} \mathbf{L}^T
$$

Thus, if our data matrix is of full rank, in other words, if the matrix $X^TX$ is positive definite, then the Normal equation can be written as follows:

$$
\begin{align*}
\mathbf{X}^{T}\mathbf{X}\beta & =\mathbf{X}^{T}y\\
\Rightarrow\mathbf{L}\mathbf{L}^{T}\beta & =\mathbf{X}^{T}y\\
\tag{1}\Rightarrow\mathbf{L}\left(\mathbf{L}^{T}\beta\right) & =\mathbf{X}^{T}y
\end{align*}
$$

Note that the matrix $\mathbf{X}^T \mathbf{X}$ is always symmetric. Thus, the $\beta$ vector can be obtained by the following two steps:

#. Solve Lz=X'y for z (forward solve lower triangular system).
#. Solve L'b=z for b (backward solve upper triangular system).

Is there any benefit of using the upper or lower triangular matrix in solving the equation? Yes. There exist nice and fast functions in `R` to solve a system of linear equations when the matrix in the equation is an upper or lower triangular marix: [`backsolve` and `forwardsolve`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/backsolve.html). Also, look at the algorithm for finding the inverse matrix of triangular matrices of any size $n$ by $n$. Here are [link 1](http://www.mcs.csueastbay.edu/~malek/TeX/Triangle.pdf) and [link 2](https://math.stackexchange.com/questions/1003801/inverse-of-an-invertible-upper-triangular-matrix-of-order-3). As the number of variables used in linear regressions has increased over the years, finding an efficient way of solving the Normal equation has become a very important topic.

## Implementation in `R`

Let us confirm whether the choesky decomposition method works using the `chol()` function in `R`.

```{r}
library(matrixcalc)

crossprod(X) %>% is.positive.definite
crossprod(X) %>% eigen(symmetric = TRUE) %$% values > 0
```

To check out whether the matrix $\mathbf{X}^T \mathbf{X}$ is positive definite or not, we can either use the `is.positiv.definite()` function in `matrixcalc` package or use the basic functions in `R` for checking that the signs of the eigenvalues are positive. Since the matrix is positive definite, we can apply the Cholesky decomposition to $\mathbf{X}^T \mathbf{X}$. The following function calculates the regression coefficients using the Cholesky decomposition:

```{r}
# X always includes a constant vector
regCef_chol <- function(X, y){

    a <- crossprod(X)
    b <- crossprod(X, y)
    
    # cholesky decomposition
    upper <- chol(a)
    
    # we need to do the following
    # z <- forwardsolve( t(upper), b )
    # but transpose can be avoid by using
    # backsolve & transpose option = TRUE
    z <- backsolve( upper, b, transpose = TRUE)
    cef <- as.vector(backsolve(upper, z))
    
    cef
}
```

Here are some comments about the `regCef_chol()` function:

1. In `R`, you **must** use the code `crossprod(X)` rather than the `t(X) %*% X` becasue of the [efficiency](https://stackoverflow.com/questions/42773206/when-is-crossprod-preferred-to-and-when-isnt).
1. The `chol()` function returns an upper triangle matrix. Thus, the code was adjusted for using the upper tringle matrix.
1. We need to use the functions `forwardsolve` and `backsolve` if we want to implement equation (1). However, the `transpose` option allows us to use `backsolve` twice so that we can avoid the transpose. Let us take a look at the following code.

```{r}
a <- crossprod(X)
b <- crossprod(X, y)

# cholesky decomposition
upper <- chol(a)
microbenchmark(
    forwardsolve( t(upper), b ),
    backsolve( upper, b, transpose = TRUE)
)


z1 <- forwardsolve( t(upper), b )
z2 <- backsolve( upper, b, transpose = TRUE)

z1 == z2
```

Let us save the Cholesky decomposition-based coefficients of the regression to the `cef_chol` variable.

```{r}
cef_chol <- regCef_chol(X, y)
cef_chol
```

## QR decomposition

For a real-valued $n \times p$ matrix $A$, where $n \geq p$, we can decompose the matrix $A$ into the following product of two matrices:

$$
A = QR
$$
where the matrix $Q$ is a $n \times p$ matrix with orthonormal columns which satisfies $Q^TQ=I_p$. If the matrix $A$ is square, then $Q$ is orthogonal. Also, the matrix $R$ is an $p \times p$ upper triangular matrix whose diagonal elements are nonzero, which implies that $R$ is nonsingular.

The QR decomposition is preferred to the Cholesky decomposition when it comes to the calculation of regression coefficients because

1. The QR decomposition can be applied to matrices of any size, and it usually provides a more stable calculation.
1. More importantly, in the Normal equation solving process, we don't need to calculate the term, $\mathbf{X}^T \mathbf{X}$, which can be seen as follows:

$$
\begin{eqnarray*}
X^{T}X\mathbf{\beta} & = & X^{T}\mathbf{y}\\
\Rightarrow\left(QR\right)^{T}\left(QR\right)\mathbf{\beta} & = & \left(QR\right)^{T}\mathbf{y}\\
\Rightarrow R^{T}Q^{T}QR\mathbf{\beta} & = & R^{T}Q^{T}\mathbf{y}\\
\Rightarrow R^{T}R\mathbf{\beta} & = & R^{T}Q^{T}\mathbf{y} \\
\Rightarrow R\mathbf{\beta} & = & Q^{T}\mathbf{y}
\end{eqnarray*}
$$

Note that we used the properties of the matrices $Q$ and $R$ to reduce the equation above. From the last equation we can see that the beta vector is actually the solution of the linear system

$$
X \mathbf{\beta}=\mathbf{y}
$$
using the QR factorization of the matrix $X$. Thus, to calculate $\beta$, we can use `backsolve()` only one time after applying the QR decomposition to the matrix $X$.

### QR decomposition in `R`

If you use the `qr()` function in `R`, you will be a little confused when you see the result of the function:

```{r}
QRmat <- qr(X)
QRmat
```

The help page for the `qr()` function has the answer for this:

The upper triangle contains the $\bold{R}$ matrix of the decomposition and the lower triangle contains information on the $\bold{Q}$ matrix of the decomposition (stored in compact form).

TO get the $Q$ and $R$ matrices seperately, we need to use the `qr.Q()` and `qr.R()` functions, respectively.

```{r}
Qmat <- qr.Q(QRmat)
Rmat <- qr.R(QRmat)
Qmat; Rmat;
```

Thus, in `R` we can obtain the QR factorization-based coefficients as follows:

```{r}
as.vector(backsolve(Rmat, crossprod(Qmat, y)))
```

However, `R` provides the following compact solve function for the QR factorization.

```{r}
cef_qr <- qr.solve(X, y) # or solve.qr(QRmat, y)
cef_qr
```

## Result comparison

Let us compare the results between the Cholesky-based  and the QR-based coefficients using the `gmp` package coefficients as a reference value.

```{r}
sum((cef_exact - cef_chol)^2)
sum((cef_exact - cef_qr)^2)
```

We can see that the QR decomposition coefficients are closer to the `gmp` coefficients than the one based on the Cholesky decomposition. In general, QR is more stable and faster than Cholesky, which is usually said to be unstable when the positive definite matrix has one or more eigenvalues close to zero. As we can see from the following `R` code, the last eigenvalue is close to zero, which makes the Cholesky-based coefficients unstable. 

```{r}
crossprod(X) %>% is.positive.definite
crossprod(X) %>% eigen(symmetric = TRUE) %$% values
```

For these reasons, the `lm` function in `R` uses the QR factorization-based calculation for the regression coefficients.

```{r}
cef_lm <- lm(y~0+X)$coefficients
cef_lm == cef_qr
```

### Reference

[1] Luke Tierney, STAT:7400 Computer Intensive Statistics Note:
http://homepage.divms.uiowa.edu/~luke/classes/STAT7400/notes.pdf

[2] J. W. Longley (1967) An appraisal of least-squares programs from the point of view of the user. Journal of the American Statistical Association 62, 819â€“841.

[3] The QR Decomposition of a Matrix in R: http://astrostatistics.psu.edu/su07/R/html/base/html/qr.html

[4] Monahan, John F. A primer on linear models. CRC Press, 2008.

[5] [Cholesky Factorization](https://www.value-at-risk.net/cholesky-factorization/) written by Glyn A. Holton
